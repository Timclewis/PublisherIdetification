{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "707bab6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.5.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras as k\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random as r\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imghdr\n",
    "import os\n",
    "import datetime\n",
    "import PIL\n",
    "import io\n",
    "import sklearn\n",
    "import itertools\n",
    "\n",
    "from keras import backend\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "from tensorflow.keras.applications import ResNet50V2, Xception, EfficientNetB3, EfficientNetB4, EfficientNetB5\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from collections import Counter\n",
    "from PIL import ImageFile, Image, ImageOps\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "print(f'Tensorflow version {tf.version.VERSION}')\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f282d503",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1f337173",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PATH_TRAIN = 'C:/Users/crims/Tensorflow/PublisherIdetification/realpublishers/training/'\n",
    "PATH_TEST = 'C:/Users/crims/Tensorflow/PublisherIdetification/realpublishers/testing/'\n",
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, val_split, height, width, batch_size, steps):\n",
    "        self.val_split = val_split\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = steps\n",
    "        \n",
    "    def train_generator(self):    \n",
    "        train_generator = ImageDataGenerator(rescale=1./255, validation_split=self.val_split)\n",
    "        print('Training folder:')\n",
    "        self.train_data = train_generator.flow_from_directory(PATH_TRAIN, target_size=(self.height, self.width),\n",
    "                                                     class_mode = 'categorical', batch_size=self.batch_size, \n",
    "                                                     subset = 'training')\n",
    "        print('Validation folder:')\n",
    "        self.valid_data = train_generator.flow_from_directory(PATH_TRAIN, target_size=(self.height, self.width),\n",
    "                                                     class_mode = 'categorical', batch_size=self.batch_size, \n",
    "                                                     subset = 'validation')\n",
    "\n",
    "    def test_generator(self):\n",
    "        test_generator = ImageDataGenerator(rescale=1./255)\n",
    "        print('Test folder:')\n",
    "        self.test_data = test_generator.flow_from_directory(PATH_TEST, target_size=(self.width, self.height), \n",
    "                                                   class_mode=None, batch_size=1, shuffle=False)\n",
    "        print()\n",
    "        \n",
    "    def generator_info(self):\n",
    "        # Print info about the generated data\n",
    "        self.class_num = self.train_data.num_classes\n",
    "        self.class_counter = list(Counter(self.train_data.classes).values())\n",
    "        self.class_counter_valid = list(Counter(self.valid_data.classes).values())\n",
    "        self.class_names = list(self.train_data.class_indices)\n",
    "        self.labels = self.train_data.labels\n",
    "        print(f'Train data class name and num {dict(zip(self.class_names, self.class_counter))}')\n",
    "        print(f'Valid data class name and num {dict(zip(self.class_names, self.class_counter_valid))}')\n",
    "        print(f'Num files trained {self.batch_size * self.steps} and validated {(self.batch_size * self.steps * self.val_split):.0f} per epoch')\n",
    "        print(f'Images resized to {self.height}x{self.width} trained avg {(self.train_data.n  // (self.batch_size * self.steps)):.1f} epochs' )\n",
    "        print()\n",
    "        \n",
    "    def clean_data(self):\n",
    "        # Remove nontype files from data folder (run once EVER for data)\n",
    "        filenames = self.test_data.filenames\n",
    "        n = 0\n",
    "        while n < self.test_data.n:\n",
    "            path = f'{PATH_TRAIN}{filenames[n]}'\n",
    "            image = cv2.imread(path)\n",
    "            img_type = imghdr.what(path)\n",
    "            if img_type != \"jpeg\":\n",
    "                print(f'Removing image from {path}')\n",
    "                os.remove(path)\n",
    "                n += 1\n",
    "            else:\n",
    "                n += 1\n",
    "        print('All done!') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d8d1d8",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a103b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensorboard:\n",
    "    #Code from Tensorflow tutorial Tensorboard\n",
    "    @staticmethod\n",
    "    def plot_to_image(figure):\n",
    "      # Save the plot to a PNG in memory.\n",
    "      buf = io.BytesIO()\n",
    "      plt.savefig(buf, format='png')\n",
    "      plt.close(figure)\n",
    "      buf.seek(0)\n",
    "      # Convert PNG buffer to TF image\n",
    "      image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "      # Add the batch dimension\n",
    "      image = tf.expand_dims(image, 0)\n",
    "      return image\n",
    "    \n",
    "    @staticmethod           \n",
    "    def plot_confusion_matrix(cm, class_names):\n",
    "        # Normalize the confusion matrix.\n",
    "        cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "        # Use white text if squares are dark; otherwise black.\n",
    "        threshold = cm.max() / 2.\n",
    "        figure = plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Greens, vmin=0, vmax=1)\n",
    "        plt.title(\"Confusion matrix\")\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names, rotation=45)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "            plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        return figure\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_img(img_path):\n",
    "        #img = Image.open(StringIO(img_path))\n",
    "        img = tf.io.read_file(img_path)\n",
    "        # convert the compressed string to a 3D uint8 tensor\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        # resize the image to the desired size for your model\n",
    "        img = tf.image.resize(img, (200, 300))\n",
    "        return img\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1251049c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "81fff37a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class BuildModel(DataGenerator):\n",
    "    def __init__(self, data_generator, network, pooling, optimizer, learn_rate, epochs, samples):\n",
    "        self.width = data_generator.width\n",
    "        self.height = data_generator.height\n",
    "        self.class_num = data_generator.class_num\n",
    "        self.class_names = data_generator.class_names\n",
    "        self.steps = data_generator.steps\n",
    "        self.val_steps = data_generator.steps * data_generator.val_split\n",
    "        self.batch_size = data_generator.batch_size\n",
    "        self.train_data = data_generator.train_data\n",
    "        self.valid_data = data_generator.valid_data\n",
    "        self.test_data = data_generator.test_data\n",
    "        \n",
    "        self.network = network\n",
    "        self.lr = learn_rate\n",
    "        self.opt = optimizer\n",
    "        self.pool = pooling\n",
    "        self.epochs = epochs\n",
    "        self.samples = samples\n",
    "        self.publisher_names = list(self.train_data.class_indices.keys())[0:self.class_num]\n",
    "        \n",
    "        # Create labels list\n",
    "        self.test_num = self.test_data.n / self.class_num\n",
    "        self.test_labels =  [0,1,2,3] * int(self.test_num) #np.repeat(list(range(0,(self.class_num))), self.test_num)\n",
    "    \n",
    "    def compile_model(self):\n",
    "        base = self.network(include_top=False, weights='imagenet', \n",
    "                     input_shape=(self.width, self.height, 3), pooling=self.pool)\n",
    "        opt = self.opt(learning_rate=self.lr)  \n",
    "        x = base.output\n",
    "        #x = layers.Dense(512, activation= 'relu')(x)\n",
    "        x = layers.Dense(self.class_num, activation='softmax')(x)\n",
    "        self.model = tf.keras.Model(base.input, x)\n",
    "        self.model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer=opt)\n",
    "        \n",
    "        self.embeddings = tf.keras.Model(inputs=self.model.inputs, \n",
    "                                         outputs=self.model.layers[-2].output)\n",
    "    \n",
    "    def run_model(self, summary, proj, save): \n",
    "        if summary == True:\n",
    "            self.model.summary()\n",
    "            \n",
    "        self.current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "        self.log_dir = os.path.join(f'tensorboard_logs/{self.network.__name__}{self.pool}_'\n",
    "                                    f'{self.opt.__name__}lr{self.lr}_E{self.epochs}'\n",
    "                                    f'B{self.batch_size}-{self.current_time}')\n",
    "                                    \n",
    "        cm_callback = k.callbacks.LambdaCallback(on_epoch_end= self.log_confusion_matrix)\n",
    "        image_callback = k.callbacks.LambdaCallback(on_epoch_end= self.log_images)\n",
    "        projector_callback = k.callbacks.LambdaCallback(on_train_end=self.log_projection)\n",
    "        scalar_callback = k.callbacks.TensorBoard(log_dir = self.log_dir, update_freq=100, \n",
    "                                                  histogram_freq=0, profile_batch=100,\n",
    "                                                  write_graph=True, write_images=True)\n",
    "        \n",
    "        self.file_writer_cm = tf.summary.create_file_writer(f'{self.log_dir}/cm')\n",
    "        self.file_writer_image = tf.summary.create_file_writer(f'{self.log_dir}/image')\n",
    "\n",
    "        self.model.fit(self.train_data, validation_data= self.valid_data, \n",
    "                       callbacks=[scalar_callback, cm_callback, image_callback, projector_callback], \n",
    "                       epochs=self.epochs, steps_per_epoch=self.steps, \n",
    "                       validation_steps=self.val_steps * 2,\n",
    "                       batch_size=self.batch_size, verbose=1)\n",
    "        \n",
    "        if save == True:\n",
    "            self.model.save(f'{self.network.__name__}_publisherid-{self.current_time}')\n",
    "\n",
    "    def log_projection(self, logs):\n",
    "        sprite_width = 100\n",
    "        sprite_height = 150\n",
    "        datapoints = 500\n",
    "        logdir = os.path.join('tensorboard_logs')\n",
    "        if not os.path.exists(logdir):\n",
    "            os.makedirs(logdir)\n",
    "\n",
    "        # Generate embeddings\n",
    "        images_pil = []\n",
    "        images_embeddings = []\n",
    "        labels = []\n",
    "        for x in list(r.sample(range(0,999), datapoints)):\n",
    "            img_path = self.test_data.filepaths[x]\n",
    "            img_tf = Tensorboard.get_img(img_path)\n",
    "\n",
    "            # Save both tf image for prediction and PIL image for sprite\n",
    "            img_pil = np.array(Image.open(img_path).resize((sprite_width, sprite_height)))\n",
    "            img_embedding = self.embeddings(tf.expand_dims(img_tf, axis=0))\n",
    "            images_embeddings.append(np.array(img_embedding[0]))\n",
    "            images_pil.append(img_pil)\n",
    "\n",
    "            # create and store labels\n",
    "            label = img_path[77:-4] \n",
    "            labels.append(label)      \n",
    "            with open(os.path.join(f'{logdir}/projector/', 'metadata.tsv'), 'w') as f: \n",
    "                 for label in labels:\n",
    "                    f.write(f'{label}\\n')\n",
    "\n",
    "        one_square_size = int(np.ceil(np.sqrt(len(images_embeddings))))\n",
    "        tile_width = sprite_width * one_square_size\n",
    "        tile_height = sprite_height * one_square_size\n",
    "        spriteimage = Image.new(mode='RGBA', size=(tile_width, tile_height), color=(0,0,0,0))\n",
    "\n",
    "        for count, image in enumerate(images_pil):\n",
    "            div, mod = divmod(count, one_square_size)\n",
    "            h_loc = sprite_height * div\n",
    "            w_loc = sprite_width * mod\n",
    "            image = Image.fromarray(image)\n",
    "            spriteimage.paste(image, (w_loc, h_loc, w_loc+sprite_width, h_loc+sprite_height))\n",
    "\n",
    "        spriteimage.save(os.path.join(f'{logdir}/projector/', 'sprite.png'))\n",
    "        feature_vector = tf.Variable(images_embeddings)\n",
    "        checkpoint = tf.train.Checkpoint(embedding=feature_vector)\n",
    "        checkpoint.save(os.path.join(f'{logdir}/projector/', 'embedding.ckpt'))\n",
    "\n",
    "        # Set up config\n",
    "        config = projector.ProjectorConfig()\n",
    "        embedding = config.embeddings.add( )\n",
    "        embedding.tensor_name = 'embedding/.ATTRIBUTES/VARIABLE_VALUE'\n",
    "        embedding.metadata_path = 'projector/metadata.tsv'\n",
    "        embedding.sprite.image_path = 'projector/sprite.png'\n",
    "        embedding.sprite.single_image_dim.extend((sprite_width, sprite_height))\n",
    "        projector.visualize_embeddings(logdir, config)\n",
    "            \n",
    "    def log_confusion_matrix(self, epoch, logs):  \n",
    "        # Use the model to predict the values from the test_images.\n",
    "        test_pred_raw = self.model.predict(self.test_data)\n",
    "        test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "\n",
    "        # Calculate the confusion matrix using sklearn.metrics\n",
    "        cm = sklearn.metrics.confusion_matrix(self.test_labels, test_pred)\n",
    "        figure = Tensorboard.plot_confusion_matrix(cm, class_names=self.class_names)\n",
    "        cm_image = Tensorboard.plot_to_image(figure)\n",
    "\n",
    "        # Log the confusion matrix as an image summary.\n",
    "        with self.file_writer_cm.as_default():\n",
    "            tf.summary.image(f'{self.network.__name__}-{self.current_time}', cm_image, step=epoch)\n",
    "            \n",
    "    def log_images(self, epoch, logs):\n",
    "        # Create pubs list\n",
    "        self.pubs = list(r.sample(range(0, 249), self.samples)) \n",
    "        self.pubs.extend(r.sample(range(250, 499), self.samples))\n",
    "        self.pubs.extend(r.sample(range(500, 749), self.samples))\n",
    "        self.pubs.extend(r.sample(range(750, 999), self.samples))\n",
    "        # Create image paths from test_data\n",
    "        self.data = []\n",
    "        for i in self.pubs:\n",
    "            img = load_img(self.test_data.filepaths[i])\n",
    "            img = img.resize((self.width, self.height)) # width x height\n",
    "            img_arr = np.asarray(img)\n",
    "            self.data.append(img_arr)\n",
    "            \n",
    "        # Data should be in (BATCH_SIZE, H, W, C) \n",
    "        assert np.size(np.shape(self.data)) == 4\n",
    "        # Create a figure to contain the plot.\n",
    "        figure = plt.figure(figsize=(10,10))\n",
    "        num_images = np.shape(self.data)[0]\n",
    "        size = int(np.ceil(np.sqrt(num_images)))\n",
    "\n",
    "        for i in range(len(self.pubs)):\n",
    "            # Start next subplot.\n",
    "            plt.subplot(size, size, i + 1, title=self.class_names[self.test_labels[self.pubs[i]]])\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.grid(False)\n",
    "            plt.imshow(self.data[i], cmap=plt.cm.binary)\n",
    "\n",
    "        with self.file_writer_image.as_default():        \n",
    "                tf.summary.image(f'{self.network.__name__}-{self.current_time}', \n",
    "                                 Tensorboard.plot_to_image(figure), max_outputs=len(self.pubs), step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e91e6d01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training folder:\n",
      "Found 8830 images belonging to 4 classes.\n",
      "Validation folder:\n",
      "Found 1556 images belonging to 4 classes.\n",
      "Test folder:\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "Train data class name and num {'IEEE': 1009, 'Macmillan': 3041, 'Springer Nature': 3465, 'Wolters Kluwer Health': 1315}\n",
      "Valid data class name and num {'IEEE': 177, 'Macmillan': 536, 'Springer Nature': 611, 'Wolters Kluwer Health': 232}\n",
      "Num files trained 1000 and validated 150 per epoch\n",
      "Images resized to 300x200 trained avg 8.0 epochs\n",
      "\n",
      "Epoch 1/20\n",
      " 15/100 [===>..........................] - ETA: 36s - loss: 1.3104 - acc: 0.4333"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[512,3072,1,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_36/block7c_project_conv/Conv2D (defined at <ipython-input-124-566e4944beb0>:58) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Func/cond/then/_0/input/_22/_270]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[512,3072,1,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_36/block7c_project_conv/Conv2D (defined at <ipython-input-124-566e4944beb0>:58) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_5127834]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-a28f66a00abf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmod1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBuildModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdat1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEfficientNetB5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'avg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5e-5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#xception ~ 5e-4, efficient ~ 8e-5, Resnet ~ 5e-5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmod1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmod1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-124-566e4944beb0>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(self, summary, proj, save)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_writer_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_file_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{self.log_dir}/image'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         self.model.fit(self.train_data, validation_data= self.valid_data, \n\u001b[0m\u001b[0;32m     59\u001b[0m                        \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mscalar_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprojector_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                        \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[512,3072,1,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_36/block7c_project_conv/Conv2D (defined at <ipython-input-124-566e4944beb0>:58) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Func/cond/then/_0/input/_22/_270]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[512,3072,1,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_36/block7c_project_conv/Conv2D (defined at <ipython-input-124-566e4944beb0>:58) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_5127834]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "#Args: valid split, width, height, batch_size, steps\n",
    "# batch_size = 32 for Xception and 12 for efficientB5, 16 for B4, 24 for B3\n",
    "dat1 = DataGenerator(0.15, 300, 200, 8, 100)\n",
    "dat1.train_generator()\n",
    "dat1.test_generator()\n",
    "dat1.generator_info()\n",
    "\n",
    "# If the position of objects is important Avg pool if not Max Pooling \n",
    "# Args: obj, network, pooling, optimizer, learn_rate, epochs, samples\n",
    "mod1 = BuildModel(dat1, EfficientNetB5, 'avg', Adam, 5e-5, 20, 4) #xception ~ 5e-4, efficient ~ 8e-5, Resnet ~ 5e-5\n",
    "mod1.compile_model()\n",
    "mod1.run_model(False, True, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6e90a6",
   "metadata": {},
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "50ca5394",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Predictor(BuildModel):\n",
    "    def __init__(self, build_model):\n",
    "        self.test_data = build_model.test_data\n",
    "        self.valid_data = build_model.valid_data\n",
    "        self.test_labels = build_model.test_labels\n",
    "        self.width = build_model.width\n",
    "        self.height = build_model.height\n",
    "        self.model = build_model.model\n",
    "        self.class_names = build_model.class_names\n",
    "        self.batch_size = build_model.batch_size\n",
    "             \n",
    "    def img_predict(self):\n",
    "        n = r.randint(0, self.test_data.n)\n",
    "        filenames = self.valid_data.filenames\n",
    "        path = f'{PATH_TRAIN}{filenames[n]}'\n",
    "        pic = mpimg.imread(path)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(pic)\n",
    "        plt.show()\n",
    "\n",
    "        img = tf.keras.preprocessing.image.load_img(path, target_size=(self.width, self.height))\n",
    "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img_batch = np.expand_dims(img_array, axis=0)\n",
    "        img_processed = tf.keras.applications.xception.preprocess_input(img_batch)\n",
    "\n",
    "        prediction = self.model(img_processed, training=False)\n",
    "        Top_index = np.argsort(np.max(prediction, axis=0))[-1]\n",
    "        Second_index = np.argsort(np.max(prediction, axis=0))[-2]\n",
    "\n",
    "        sort = np.sort(max(prediction))\n",
    "        print(f'Prediction {self.class_names[Top_index]} with conf {round(sort[len(sort) - 1]*100)}%')\n",
    "        print(f'2nd predict {self.class_names[Second_index]} with conf {round(sort[len(sort) - 2] * 100)}%')           \n",
    "        print(f'Answer is {filenames[n][:]}')\n",
    "        \n",
    "    def batch_evaluate(self, num_batches):\n",
    "        self.steps = num_batches\n",
    "        self.model.evaluate(self.valid_data, batch_size=self.batch_size, steps=self.steps)\n",
    "        \n",
    "    def batch_predict(self, num_predicts):\n",
    "        i = num_predicts\n",
    "        test_pred_raw = self.model.predict(self.test_data)\n",
    "        test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "        test_labels = self.test_labels\n",
    "        acc = sum(1 for x,y in zip(test_pred[0:i], test_labels[0:i]) if x == y) / len(test_labels[0:i])\n",
    "        print(f'Accuracy of predictions is {acc*100}%')        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5a305e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of predictions is 72.5%\n"
     ]
    }
   ],
   "source": [
    "pred1 = Predictor(mod1)\n",
    "#pred1.img_predict()\n",
    "pred1.batch_predict(1000)\n",
    "#pred1.batch_evaluate(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43961d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
