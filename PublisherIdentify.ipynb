{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "707bab6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.5.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras as k\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random as r\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imghdr\n",
    "import os\n",
    "import datetime\n",
    "import PIL\n",
    "import io\n",
    "import sklearn\n",
    "import itertools\n",
    "\n",
    "from keras import backend\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "from tensorflow.keras.applications import ResNet50V2, Xception, EfficientNetB3, EfficientNetB4, EfficientNetB5\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from collections import Counter\n",
    "from PIL import ImageFile, Image, ImageOps\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "print(f'Tensorflow version {tf.version.VERSION}')\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f282d503",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f337173",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PATH_TRAIN = 'C:/Users/crims/Tensorflow/PublisherIdetification/realpublishers/training/'\n",
    "PATH_TEST = 'C:/Users/crims/Tensorflow/PublisherIdetification/realpublishers/testing/'\n",
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, val_split, height, width, batch_size, steps):\n",
    "        self.val_split = val_split\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = steps\n",
    "        \n",
    "    def train_generator(self):    \n",
    "        train_generator = ImageDataGenerator(rescale=1./255, validation_split=self.val_split)\n",
    "        print('Training folder:')\n",
    "        self.train_data = train_generator.flow_from_directory(PATH_TRAIN, target_size=(self.height, self.width),\n",
    "                                                     class_mode = 'categorical', batch_size=self.batch_size, \n",
    "                                                     subset = 'training')\n",
    "        print('Validation folder:')\n",
    "        self.valid_data = train_generator.flow_from_directory(PATH_TRAIN, target_size=(self.height, self.width),\n",
    "                                                     class_mode = 'categorical', batch_size=self.batch_size, \n",
    "                                                     subset = 'validation')\n",
    "\n",
    "    def test_generator(self):\n",
    "        test_generator = ImageDataGenerator(rescale=1./255)\n",
    "        print('Test folder:')\n",
    "        self.test_data = test_generator.flow_from_directory(PATH_TEST, target_size=(self.width, self.height), \n",
    "                                                   class_mode=None, batch_size=1, shuffle=False)\n",
    "        print()\n",
    "        \n",
    "    def generator_params(self):\n",
    "        # Print info about the generated data\n",
    "        self.class_num = self.train_data.num_classes\n",
    "        self.class_counter = list(Counter(self.train_data.classes).values())\n",
    "        self.class_counter_valid = list(Counter(self.valid_data.classes).values())\n",
    "        self.class_names = list(self.train_data.class_indices)\n",
    "        self.labels = self.train_data.labels\n",
    "        print(f'Train data class name and num {dict(zip(self.class_names, self.class_counter))}')\n",
    "        print(f'Valid data class name and num {dict(zip(self.class_names, self.class_counter_valid))}')\n",
    "        print(f'Num files trained {self.batch_size * self.steps} and validated {(self.batch_size * self.steps * self.val_split):.0f} per epoch')\n",
    "        print(f'Images resized to {self.height}x{self.width} trained avg {(self.train_data.n  // (self.batch_size * self.steps)):.1f} epochs' )\n",
    "        print()\n",
    "        \n",
    "    def clean_data(self):\n",
    "        # Remove nontype files from data folder (run once EVER for data)\n",
    "        filenames = self.test_data.filenames\n",
    "        n = 0\n",
    "        while n < self.test_data.n:\n",
    "            path = f'{PATH_TRAIN}{filenames[n]}'\n",
    "            image = cv2.imread(path)\n",
    "            img_type = imghdr.what(path)\n",
    "            if img_type != \"jpeg\":\n",
    "                print(f'Removing image from {path}')\n",
    "                os.remove(path)\n",
    "                n += 1\n",
    "            else:\n",
    "                n += 1\n",
    "        print('All done!') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d8d1d8",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a103b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    #Code from Tensorflow tutorial Tensorboard\n",
    "    @staticmethod\n",
    "    def plot_to_image(figure):\n",
    "      # Save the plot to a PNG in memory.\n",
    "      buf = io.BytesIO()\n",
    "      plt.savefig(buf, format='png')\n",
    "      plt.close(figure)\n",
    "      buf.seek(0)\n",
    "      # Convert PNG buffer to TF image\n",
    "      image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "      # Add the batch dimension\n",
    "      image = tf.expand_dims(image, 0)\n",
    "      return image\n",
    "    \n",
    "    @staticmethod           \n",
    "    def plot_confusion_matrix(cm, class_names):\n",
    "        # Normalize the confusion matrix.\n",
    "        cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "        # Use white text if squares are dark; otherwise black.\n",
    "        threshold = cm.max() / 2.\n",
    "        figure = plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Greens, vmin=0, vmax=1)\n",
    "        plt.title(\"Confusion matrix\")\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names, rotation=45)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "            plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        return figure\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_img(img_path):\n",
    "        #img = Image.open(StringIO(img_path))\n",
    "        img = tf.io.read_file(img_path)\n",
    "        # convert the compressed string to a 3D uint8 tensor\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        # resize the image to the desired size for your model\n",
    "        img = tf.image.resize(img, (200, 300))\n",
    "        return img\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1251049c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81fff37a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class BuildModel(DataGenerator):\n",
    "    def __init__(self, data_generator, network, pooling, optimizer, learn_rate, epochs):\n",
    "        self.width = data_generator.width\n",
    "        self.height = data_generator.height\n",
    "        self.class_num = data_generator.class_num\n",
    "        self.class_names = data_generator.class_names\n",
    "        self.steps = data_generator.steps\n",
    "        self.val_steps = data_generator.steps * data_generator.val_split\n",
    "        self.batch_size = data_generator.batch_size\n",
    "        self.train_data = data_generator.train_data\n",
    "        self.valid_data = data_generator.valid_data\n",
    "        self.test_data = data_generator.test_data\n",
    "        \n",
    "        self.network = network\n",
    "        self.lr = learn_rate\n",
    "        self.opt = optimizer\n",
    "        self.pool = pooling\n",
    "        self.epochs = epochs\n",
    "        self.samples = samples\n",
    "        self.publisher_names = list(self.train_data.class_indices.keys())[0:self.class_num]\n",
    "            \n",
    "    def compile_model(self):\n",
    "        base = self.network(include_top=False, weights='imagenet', \n",
    "                     input_shape=(self.width, self.height, 3), pooling=self.pool)\n",
    "        opt = self.opt(learning_rate=self.lr)  \n",
    "        x = base.output\n",
    "        #x = layers.Dense(512, activation= 'relu')(x)\n",
    "        x = layers.Dense(self.class_num, activation='softmax')(x)\n",
    "        self.model = tf.keras.Model(base.input, x)\n",
    "        self.model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer=opt)\n",
    "        \n",
    "        self.embeddings = tf.keras.Model(inputs=self.model.inputs, \n",
    "                                         outputs=self.model.layers[-2].output)\n",
    "    \n",
    "    def run_model(self, summary, confusion_matrix, image_visual, projector): \n",
    "        if summary == True:\n",
    "            self.model.summary()\n",
    "            \n",
    "        self.current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "        self.log_dir = os.path.join(f'tensorboard_logs/{self.network.__name__}{self.pool}_'\n",
    "                                    f'{self.opt.__name__}lr{self.lr}_E{self.epochs}'\n",
    "                                    f'B{self.batch_size}-{self.current_time}')\n",
    "        if confusion_matrix == True:\n",
    "            cm_callback = k.callbacks.LambdaCallback(on_epoch_end= self.log_confusion_matrix)\n",
    "            \n",
    "        if image_visual == True:\n",
    "            image_callback = k.callbacks.LambdaCallback(on_epoch_end= self.log_images)\n",
    "            \n",
    "        if projector == True:\n",
    "            projector_callback = k.callbacks.LambdaCallback(on_train_end=self.log_projection)\n",
    "        \n",
    "        scalar_callback = k.callbacks.TensorBoard(log_dir = self.log_dir, update_freq=100, \n",
    "                                                  histogram_freq=0, profile_batch=100,\n",
    "                                                  write_graph=True, write_images=True)\n",
    "        \n",
    "        self.file_writer_cm = tf.summary.create_file_writer(f'{self.log_dir}/cm')\n",
    "        self.file_writer_image = tf.summary.create_file_writer(f'{self.log_dir}/image')\n",
    "\n",
    "        self.model.fit(self.train_data, validation_data= self.valid_data, \n",
    "                       callbacks=[scalar_callback, cm_callback, image_callback, projector_callback], \n",
    "                       epochs=self.epochs, steps_per_epoch=self.steps, \n",
    "                       validation_steps=self.val_steps * 2,\n",
    "                       batch_size=self.batch_size, verbose=1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1.model.save(f'{self.network.__name__}_publisherid-{self.current_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f1b6a",
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa3c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(BuildModel):\n",
    "    def __init__(self, build_model):\n",
    "        self.width = build_model.width\n",
    "        self.height = build_model.height\n",
    "        self.embeddings = build_model.embeddings\n",
    "        self.test_data = build_model.test_data\n",
    "        self.model = build_model.model\n",
    "        self.class_names = build_model.class_names\n",
    "        self.test_num = build_model.test_num\n",
    "        self.test_labels =  [0,1,2,3] * int(build_model.test_num) \n",
    "        self.current_time = build_model.current_time\n",
    "        self.network = build_model.network\n",
    "        \n",
    "    def log_projection(self, logs):\n",
    "        sprite_width = 50\n",
    "        sprite_height = 75\n",
    "        datapoints = 500\n",
    "        logdir = os.path.join('tensorboard_logs')\n",
    "        if not os.path.exists(logdir):\n",
    "            os.makedirs(logdir)\n",
    "\n",
    "        # Generate embeddings\n",
    "        images_pil = []\n",
    "        images_embeddings = []\n",
    "        labels = []\n",
    "        for x in list(r.sample(range(0,999), datapoints)):\n",
    "            img_path = self.test_data.filepaths[x]\n",
    "            img_tf = Utils.get_img(img_path)\n",
    "\n",
    "            # Save both tf image for prediction and PIL image for sprite\n",
    "            img_pil = np.array(Image.open(img_path).resize((sprite_width, sprite_height)))\n",
    "            img_embedding = self.embeddings(tf.expand_dims(img_tf, axis=0))\n",
    "            images_embeddings.append(np.array(img_embedding[0]))\n",
    "            images_pil.append(img_pil)\n",
    "\n",
    "            # create and store labels\n",
    "            label = img_path[77:-4] \n",
    "            labels.append(label)      \n",
    "            with open(os.path.join(f'{logdir}/projector/', 'metadata.tsv'), 'w') as f: \n",
    "                 for label in labels:\n",
    "                    f.write(f'{label}\\n')\n",
    "\n",
    "        one_square_size = int(np.ceil(np.sqrt(len(images_embeddings))))\n",
    "        tile_width = sprite_width * one_square_size\n",
    "        tile_height = sprite_height * one_square_size\n",
    "        spriteimage = Image.new(mode='RGBA', size=(tile_width, tile_height), color=(0,0,0,0))\n",
    "\n",
    "        for count, image in enumerate(images_pil):\n",
    "            div, mod = divmod(count, one_square_size)\n",
    "            h_loc = sprite_height * div\n",
    "            w_loc = sprite_width * mod\n",
    "            image = Image.fromarray(image)\n",
    "            spriteimage.paste(image, (w_loc, h_loc, w_loc+sprite_width, h_loc+sprite_height))\n",
    "        spriteimage.save(os.path.join(f'{logdir}/projector/', 'sprite.png'))\n",
    "        \n",
    "        feature_vector = tf.Variable(images_embeddings)\n",
    "        checkpoint = tf.train.Checkpoint(embedding=feature_vector)\n",
    "        checkpoint.save(os.path.join(f'{logdir}/projector/', 'embedding.ckpt'))\n",
    "\n",
    "        # Set up config\n",
    "        config = projector.ProjectorConfig()\n",
    "        embedding = config.embeddings.add( )\n",
    "        embedding.tensor_name = 'embedding/.ATTRIBUTES/VARIABLE_VALUE'\n",
    "        embedding.metadata_path = 'projector/metadata.tsv'\n",
    "        embedding.sprite.image_path = 'projector/sprite.png'\n",
    "        embedding.sprite.single_image_dim.extend((sprite_width, sprite_height))\n",
    "        projector.visualize_embeddings(logdir, config)\n",
    "            \n",
    "    def log_confusion_matrix(self, epoch, logs):  \n",
    "        # Use the model to predict the values from the test_images.\n",
    "        test_pred_raw = self.model.predict(self.test_data)\n",
    "        test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "\n",
    "        # Calculate the confusion matrix using sklearn.metrics\n",
    "        cm = sklearn.metrics.confusion_matrix(self.test_labels, test_pred)\n",
    "        figure = Utils.plot_confusion_matrix(cm, class_names=self.class_names)\n",
    "        cm_image = Utils.plot_to_image(figure)\n",
    "\n",
    "        # Log the confusion matrix as an image summary.\n",
    "        with self.file_writer_cm.as_default():\n",
    "            tf.summary.image(f'{self.network.__name__}-{self.current_time}', cm_image, step=epoch)\n",
    "            \n",
    "    def log_images(self, epoch, logs):\n",
    "        # Create pubs list\n",
    "        pubs = list(r.sample(range(0, 249), self.samples)) \n",
    "        pubs.extend(r.sample(range(250, 499), self.samples))\n",
    "        pubs.extend(r.sample(range(500, 749), self.samples))\n",
    "        pubs.extend(r.sample(range(750, 999), self.samples))\n",
    "        # Create image paths from test_data\n",
    "        data = []\n",
    "        for i in pubs:\n",
    "            img = load_img(self.test_data.filepaths[i])\n",
    "            img = img.resize((self.width, self.height)) # width x height\n",
    "            img_arr = np.asarray(img)\n",
    "            data.append(img_arr)\n",
    "            \n",
    "        # Data should be in (BATCH_SIZE, H, W, C) \n",
    "        assert np.size(np.shape(self.data)) == 4\n",
    "        # Create a figure to contain the plot.\n",
    "        figure = plt.figure(figsize=(10,10))\n",
    "        num_images = np.shape(self.data)[0]\n",
    "        size = int(np.ceil(np.sqrt(num_images)))\n",
    "\n",
    "        for i in range(len(self.pubs)):\n",
    "            # Start next subplot.\n",
    "            plt.subplot(size, size, i + 1, title=self.class_names[self.test_labels[self.pubs[i]]])\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.grid(False)\n",
    "            plt.imshow(self.data[i], cmap=plt.cm.binary)\n",
    "\n",
    "        with self.file_writer_image.as_default():        \n",
    "                tf.summary.image(f'{self.network.__name__}-{self.current_time}', \n",
    "                                 Tensorboard.plot_to_image(figure), max_outputs=len(self.pubs), step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e91e6d01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training folder:\n",
      "Found 8830 images belonging to 4 classes.\n",
      "Validation folder:\n",
      "Found 1556 images belonging to 4 classes.\n",
      "Test folder:\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "Train data class name and num {'IEEE': 1009, 'Macmillan': 3041, 'Springer Nature': 3465, 'Wolters Kluwer Health': 1315}\n",
      "Valid data class name and num {'IEEE': 177, 'Macmillan': 536, 'Springer Nature': 611, 'Wolters Kluwer Health': 232}\n",
      "Num files trained 1200 and validated 180 per epoch\n",
      "Images resized to 300x200 trained avg 7.0 epochs\n",
      "\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 72s 547ms/step - loss: 1.0177 - acc: 0.6092 - val_loss: 1.3619 - val_acc: 0.3500\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 50s 497ms/step - loss: 0.7062 - acc: 0.7408 - val_loss: 1.3244 - val_acc: 0.3194\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 50s 497ms/step - loss: 0.5213 - acc: 0.8242 - val_loss: 1.1769 - val_acc: 0.4833\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 50s 497ms/step - loss: 0.4507 - acc: 0.8525 - val_loss: 0.9220 - val_acc: 0.6583\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 50s 497ms/step - loss: 0.4429 - acc: 0.8542 - val_loss: 0.7326 - val_acc: 0.7556\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 50s 497ms/step - loss: 0.3511 - acc: 0.8898 - val_loss: 0.5911 - val_acc: 0.7972\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 50s 497ms/step - loss: 0.2964 - acc: 0.9017 - val_loss: 0.5287 - val_acc: 0.8111\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 50s 496ms/step - loss: 0.2927 - acc: 0.9042 - val_loss: 0.4374 - val_acc: 0.8306\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 50s 496ms/step - loss: 0.2538 - acc: 0.9242 - val_loss: 0.4271 - val_acc: 0.8389\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 50s 496ms/step - loss: 0.1904 - acc: 0.9375 - val_loss: 0.3463 - val_acc: 0.8806\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 50s 497ms/step - loss: 0.1927 - acc: 0.9342 - val_loss: 0.3233 - val_acc: 0.8889\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 50s 497ms/step - loss: 0.1740 - acc: 0.9467 - val_loss: 0.3260 - val_acc: 0.8972\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 50s 497ms/step - loss: 0.1288 - acc: 0.9633 - val_loss: 0.3154 - val_acc: 0.9056\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 50s 499ms/step - loss: 0.1622 - acc: 0.9483 - val_loss: 0.3604 - val_acc: 0.8722\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 50s 496ms/step - loss: 0.1220 - acc: 0.9608 - val_loss: 0.3796 - val_acc: 0.8750\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 50s 497ms/step - loss: 0.1541 - acc: 0.9500 - val_loss: 0.2866 - val_acc: 0.9056\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 50s 497ms/step - loss: 0.1137 - acc: 0.9658 - val_loss: 0.3965 - val_acc: 0.8667\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 50s 497ms/step - loss: 0.1204 - acc: 0.9608 - val_loss: 0.3521 - val_acc: 0.8806\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 50s 498ms/step - loss: 0.0904 - acc: 0.9708 - val_loss: 0.3715 - val_acc: 0.8889\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 50s 498ms/step - loss: 0.1091 - acc: 0.9708 - val_loss: 0.3151 - val_acc: 0.8944\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 50s 498ms/step - loss: 0.0791 - acc: 0.9775 - val_loss: 0.3318 - val_acc: 0.9000\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 50s 496ms/step - loss: 0.0651 - acc: 0.9825 - val_loss: 0.3976 - val_acc: 0.8583\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 50s 496ms/step - loss: 0.0600 - acc: 0.9817 - val_loss: 0.3424 - val_acc: 0.8778\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 50s 497ms/step - loss: 0.0690 - acc: 0.9800 - val_loss: 0.3043 - val_acc: 0.8889\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 50s 498ms/step - loss: 0.0818 - acc: 0.9750 - val_loss: 0.3432 - val_acc: 0.8833\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 50s 495ms/step - loss: 0.0664 - acc: 0.9850 - val_loss: 0.4680 - val_acc: 0.8667\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 50s 496ms/step - loss: 0.0573 - acc: 0.9817 - val_loss: 0.3431 - val_acc: 0.9139\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 50s 498ms/step - loss: 0.0372 - acc: 0.9900 - val_loss: 0.3370 - val_acc: 0.9028\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 50s 496ms/step - loss: 0.0338 - acc: 0.9942 - val_loss: 0.3139 - val_acc: 0.9000\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 50s 497ms/step - loss: 0.0403 - acc: 0.9883 - val_loss: 0.3357 - val_acc: 0.8917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crims\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: EfficientNetB5_publisherid-20210819-2131\\assets\n"
     ]
    }
   ],
   "source": [
    "#Args: valid split, width, height, batch_size, steps\n",
    "# batch_size = 32 for Xception and 12 for efficientB5, 16 for B4, 24 for B3\n",
    "dat1 = DataGenerator(0.15, 300, 200, 12, 100)\n",
    "dat1.train_generator()\n",
    "dat1.test_generator()\n",
    "dat1.generator_info()\n",
    "\n",
    "# If the position of objects is important Avg pool if not Max Pooling \n",
    "# Args: obj, network, pooling, optimizer, learn_rate, epochs, samples\n",
    "mod1 = BuildModel(dat1, EfficientNetB5, 'avg', Adam, 8e-5, 30, 4) #xception ~ 5e-4, efficient ~ 8e-5, Resnet ~ 5e-5\n",
    "mod1.compile_model()\n",
    "mod1.run_model(False, True, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56029384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crims\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: EfficientNetB5_publisherid-20210819-2131\\assets\n"
     ]
    }
   ],
   "source": [
    "mod1.model.save(f'{mod1.network.__name__}_publisherid-{mod1.current_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6e90a6",
   "metadata": {},
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50ca5394",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Predictor(BuildModel):\n",
    "    def __init__(self, build_model):\n",
    "        self.test_data = build_model.test_data\n",
    "        self.valid_data = build_model.valid_data\n",
    "        self.test_labels = build_model.test_labels\n",
    "        self.width = build_model.width\n",
    "        self.height = build_model.height\n",
    "        self.model = build_model.model\n",
    "        self.class_names = build_model.class_names\n",
    "        self.batch_size = build_model.batch_size\n",
    "             \n",
    "    def img_predict(self):\n",
    "        n = r.randint(0, self.test_data.n)\n",
    "        filenames = self.valid_data.filenames\n",
    "        path = f'{PATH_TRAIN}{filenames[n]}'\n",
    "        pic = mpimg.imread(path)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(pic)\n",
    "        plt.show()\n",
    "\n",
    "        img = tf.keras.preprocessing.image.load_img(path, target_size=(self.width, self.height))\n",
    "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img_batch = np.expand_dims(img_array, axis=0)\n",
    "        img_processed = tf.keras.applications.xception.preprocess_input(img_batch)\n",
    "\n",
    "        prediction = self.model(img_processed, training=False)\n",
    "        Top_index = np.argsort(np.max(prediction, axis=0))[-1]\n",
    "        Second_index = np.argsort(np.max(prediction, axis=0))[-2]\n",
    "\n",
    "        sort = np.sort(max(prediction))\n",
    "        print(f'Prediction {self.class_names[Top_index]} with conf {round(sort[len(sort) - 1]*100)}%')\n",
    "        print(f'2nd predict {self.class_names[Second_index]} with conf {round(sort[len(sort) - 2] * 100)}%')           \n",
    "        print(f'Answer is {filenames[n][:]}')\n",
    "        \n",
    "    def batch_evaluate(self, num_batches):\n",
    "        self.steps = num_batches\n",
    "        self.model.evaluate(self.valid_data, batch_size=self.batch_size, steps=self.steps)\n",
    "        \n",
    "    def batch_predict(self, num_predicts):\n",
    "        i = num_predicts\n",
    "        test_pred_raw = self.model.predict(self.test_data)\n",
    "        test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "        test_labels = self.test_labels\n",
    "        acc = sum(1 for x,y in zip(test_pred[0:i], test_labels[0:i]) if x == y) / len(test_labels[0:i])\n",
    "        print(f'Accuracy of predictions is {100 * acc:.1f}')        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a305e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of predictions is 78.400\n"
     ]
    }
   ],
   "source": [
    "pred1 = Predictor(mod1)\n",
    "#pred1.img_predict()\n",
    "pred1.batch_predict(1000)\n",
    "#pred1.batch_evaluate(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43961d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
